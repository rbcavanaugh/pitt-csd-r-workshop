---
title: "EXERCISE_Data Transformation, Tidying, and Wrangling"
author: "Emily Goldberg"
date: '2022-07-25'
output:
  rmdformats::html_clean:
    toc: true
    toc_depth: 2
    code_folding: show
---

# Data Transformation, Tidying, and Wrangling Excercise

The best way to learn how to use the functions that we just discussed is to use them in practice, with a specific goal in mind. Remember the case study presented to you at the beginning of our module? That case study officially is about YOU! You were given data for 5 research participants and your mentor asked you to tidy the data for next week's meeting so that you can analyze them together. 

Your mentor says they would like your final product of the aggregated data to be in long format, free of spelling/character errors, and without 2nd rater scores included. 


## About the data

Partnered with a local hospital in the town where you just started your Ph.D. program, your mentor just finished a 5 year clinical trial that investigated the efficacy of a language intervention in people with chronic post-stroke aphasia. The treatment is called Semantic Feature Analysis (SFA), and was designed primarily to improve single word noun naming in individuals with aphasia. Given that SFA is aimed at improving word retrieval, subjects were tested on their naming abilities of treatment list targets before treatment (three times), immediately after treatment (three times), and at a 2 month follow up visit (1 time). 

Every subject was treated on the same words. The duration of intervention lasted 4 weeks. Over these four weeks, participants were treated on either 2 or 3 word lists. Whether or not participants were treated on 2 vs. 3 lists depended on how successful they were in advancing through / responding to intervention. Each treatment list contains 10 items; 5 of the items were directly treated during intervention (T), and 5 were untreated (U) to gauge generalizability. The probes given multiple times before treatment, multiple times immediately after treatment, and once at a follow up time point contained a total of 30 words.

Your mentor is interested in examining changes in naming accuracy over time when comparing the time points mentioned above (pre-treatment, right after treatment, followup). For each subject, probes 1, 2, and 3 were given before treatment; probes 4, 5, and 6 after; and probe 7 at follow up. Your mentor is also interested in associating pre-treatment measures of aphasia severity and other non-language cognitive functions with naming performance before and after treatment. 

In the "data" folder that your mentor sent you, each subject has their own folder. Within the subject folders you will find a .csv file containing the probe naming data. Additionally there is a .csv file containing raw aphasia severity, verbal memory and visuospatial memory, and sustained attention scores titled "cognitive_data.csv". 

Below is a list of the variables included in the 5 "subject##_probes" .csv files: 

* Probe: (the word being probed)
* List: (the treatment list the word comes from)
* Type: (whether the word is is treated T or untreated U)
* Rater1_## and Rater2_##: (two team members alternated scoring performance - Rater 1 and Rater 2 are two different people)
* _##: (the number following "Rater#_" represents the probe number in order)


Your mentor helped you by emailing you a list of variables that the tidy version of this data should contain:

* id: (subject ID)
* item: (the word being probed)
* probe_number: (e.g. 01, 02, 03, 04, 05)
* study_phase: (if before treatment, "pre"; if right after treatment, "post"; if at followup, "followup")
* type: (treated or untreated)
* accuracy (0 or 1)
* aphasia_severity
* verbal_mem
* visual_mem

They specifically asked if the column for "id" could be the VERY FIRST column in the data frame. Also, they asked for categorical variables to be coded as factors. 

To best assist you, I have laid out a list of steps to take in order to wrangle the data. These steps will guide you through the process and assist you with selecting which commands to use. **HOWEVER** I have left out just a few intermediate steps that must be taken to successfully wrangle the data. If you are attempting to complete a step, and receive an error message, I challenge you to (1) read what the error message is telling you and (2) consider what intermediate step might need to be taken to eliminate errors. 

After you submit your final product, the module instructor is going to review your work; students with perfect products will receive a prize!

You better start reshaping these data....you got side tracked with a brutal statistics assignment and a heavy reading for your neuroscience class, and your meeting with your mentor is in 2 days!


### Load libraries:
```{r}
## INSERT CODE HERE
```


## Extreme Makover: Data Edition

To tackle this issue, we first should create code that successfully transforms one subject's data; hopefully once this is accomplished we can replicate the code with other subjects. 

### SUBJECT 01

STEP 1: Read in the probe data for Subject 1

```{r}
## INSERT CODE HERE
```

STEP 2: Become familiar with the data structure

```{r}
## INSERT CODE HERE
```

Step 3: Pivot the 10-item probe for Subject 1 from wide to long format; in this process, create a new column for probe number (the numbers are included in wide form column headers) and make sure the column containing words is named "item"

```{r}
## INSERT CODE HERE
```

Step 4: Rename the "Probe" column to be "item"

```{r}
## INSERT CODE HERE
```


Step 5: Add a column for subject ID that comes BEFORE item. 

What did you google to help you accomplish this? What google search was helpful? What google search was not helpful? Feel free to include links below: 

* Google search = helpful:
* Google search = not helpful:

```{r}
## INSERT CODE HERE
```


Step 6: Add a column that classifies if a probe was given "pre", "post", or "followup". 

```{r}
## INSERT CODE HERE
```

Step 7: Run a few checks to make sure there are no typing errors or other errors within this participant's data:

For example, we know there were 7 probes, and 30 total items. 
```{r}
7*30
```

Should be 210 observations, and there are. 

Let's set item to be a factor and make sure there are not any spelling errors there; if there are 3 lists with 10 items each, there should only be 30 levels for this factor: 

```{r}
subject1_probes_long$item <- as.factor(subject1_probes_long$item)
levels(subject1_probes_long$item)
```

There are 30 total levels. 

Let's count to make sure there are 15 * 7 trained and untrained targets: 

```{r}
15*7 ## 105

subject1_probes_long$Type <- as.factor(subject1_probes_long$Type)

subject1_probes_long %>% count(Type)
```

Things look good. Going to remove the messy data frame: 

```{r}
rm(subject1_probes)
```

### SUBJECT 02

Now that subject 1's data has been tidied, you're ready to replicate this process with subjects 2, 3, 4, and 5! Be warned...you should still carefully look at each subjects' raw messy data frame just in case slight differences exist. 

Step 8: Apply same procedure to Subject 02

Read CSV and view first several rows of data
```{r}
## INSERT CODE HERE
```

I notice the "List" variable has two periods after it. Let's rename that: 
```{r}
## INSERT CODE HERE
```

Next pivot longer: 
```{r}
## INSERT CODE HERE
```

Rename probe column: 
```{r}
## INSERT CODE HERE
```

Add column for subject id

```{r}
## INSERT CODE HERE
```

Add probe time point classification: 

```{r}
## INSERT CODE HERE
```

Check the data to see if your modifications have resulted in any fatal issues
```{r}
## INSERT CODE HERE
```

Remove the dirty data frame
```{r}
## INSERT CODE HERE
```


### SUBJECT 03

Step 9: Apply same procedure to Subject 03

Read in CSV and view head of data frame

```{r}
## INSERT CODE HERE
```

Tidy.
```{r}
## INSERT CODE HERE
```

Run some checks.

```{r}
## INSERT CODE HERE
```

Remove dirty data.
```{r}
## INSERT CODE HERE
```

### SUBJECT 04

Step 10: Apply same procedure to Subject 04

Read CSV, view head of data frame

```{r}
## INSERT CODE HERE
```

Tidy.
```{r}
## INSERT CODE HERE
```


Run some checks.

```{r}
## INSERT CODE HERE
```

Remove dirty data

```{r}
## INSERT CODE HERE
```

### SUBJECT 05

Step 11: Apply same procedure to Subject 05

Read CSV, view data head
```{r}
## INSERT CODE HERE
```

Tidy
```{r}
## INSERT CODE HERE
```

Run some checks. 

```{r}
## INSERT CODE HERE
```

Remove dirty data
```{r}
## INSERT CODE HERE
```

### Combining subject data

Step 12: Combine the subject data so there is an aggregated data frame. 

```{r}
## INSERT CODE HERE
```

You should end up with 1050 rows of 8 variables. 


### Combine aggregated naming data with cognitive data

Now we need to read in the cognitive and language data, select the columns we wish to use, and combine it with the aggregated subject naming data. 

Step 13: Read in cognitive data

```{r}
## INSERT CODE HERE
```

Step 14: Select only the variables your mentor wants to use - aphasia severity, verbal memory, and visual memory

```{r}
## INSERT CODE HERE
```


Step 15: Join using ID as the common factor!
```{r}
naming_master <- left_join(aggregated_subject, cognitive_vars_select, by = "id")
```

Did you receive an error message? If no, move on. If yes, what does the error message say, and what does it mean? What did you learn from the error message? What do you need to do to eliminate this error message and successfully join the data frames?


Step 16: Let's do a quick check on the data. What do you know about the data? How many subjects, items phase time points, etc. should there be? 

```{r}
## INSERT CODE HERE
```

If you notice any errors, fix them below; if not, skip.

```{r}
## INSERT CODE HERE
```

Step 16: Write a csv file so your work is saved

```{r}
## INSERT CODE HERE
```




